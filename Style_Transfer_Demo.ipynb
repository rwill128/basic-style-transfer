{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Style Transfer Demo.ipynb",
   "provenance": [],
   "private_outputs": true,
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from PIL import Image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WyENTFmggM19",
    "cellView": "form"
   },
   "source": [
    "#@title 4. Preprocess the images ðŸ‘¾\n",
    "#@markdown ***Don't modify this code block.***\n",
    "# Function to load an image from a file, and add a batch dimension.\n",
    "def load_img(path_to_img):\n",
    "  img = tf.io.read_file(path_to_img)\n",
    "  img = tf.io.decode_image(img, channels=3)\n",
    "  img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  img = img[tf.newaxis, :]\n",
    "\n",
    "  return img\n",
    "\n",
    "# Function to load an image from a file, and add a batch dimension.\n",
    "def load_content_img(image_pixels):\n",
    "    if image_pixels.shape[-1] == 4:\n",
    "        image_pixels = Image.fromarray(image_pixels)\n",
    "        img = image_pixels.convert('RGB')\n",
    "        img = np.array(img)\n",
    "        img = tf.convert_to_tensor(img)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = img[tf.newaxis, :]\n",
    "        return img\n",
    "    elif image_pixels.shape[-1] == 3:\n",
    "        img = tf.convert_to_tensor(image_pixels)\n",
    "        img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "        img = img[tf.newaxis, :]\n",
    "        return img\n",
    "    elif image_pixels.shape[-1] == 1:\n",
    "        raise Error('Grayscale images not supported! Please try with RGB or RGBA images.')\n",
    "    print('Exception not thrown')\n",
    "\n",
    "# Function to pre-process by resizing an central cropping it.\n",
    "def preprocess_image(image, target_dim):\n",
    "  # Resize the image so that the shorter dimension becomes 256px.\n",
    "  shape = tf.cast(tf.shape(image)[1:-1], tf.float32)\n",
    "  short_dim = min(shape)\n",
    "  scale = target_dim / short_dim\n",
    "  new_shape = tf.cast(shape * scale, tf.int32)\n",
    "  image = tf.image.resize(image, new_shape)\n",
    "\n",
    "  # Central crop the image.\n",
    "  image = tf.image.resize_with_crop_or_pad(image, target_dim, target_dim)\n",
    "\n",
    "  return image\n",
    "\n",
    "# Convert the content image from Bytes to NumPy array.\n",
    "content_image = np.array(content_image)\n",
    "\n",
    "# Load the input images.\n",
    "content_image = load_content_img(content_image)\n",
    "style_image = load_img(style_image_path)\n",
    "\n",
    "# Preprocess the input images.\n",
    "preprocessed_content_image = preprocess_image(content_image, 384)\n",
    "preprocessed_style_image = preprocess_image(style_image, 256)\n",
    "\n",
    "print('Preprocessing the style and the content images...')\n",
    "print('Style image shape:', preprocessed_style_image.shape)\n",
    "print('Content image shape:', preprocessed_content_image.shape)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lUDZZN7Gxaz3",
    "cellView": "form"
   },
   "source": [
    "#@title 5. Download the style transfer networks from TF Hub ðŸ’»\n",
    "#@markdown ***Don't modify this code block.***\n",
    "#@markdown \n",
    "\n",
    "# Download the style bottleneck and transfer networks\n",
    "print('Downloading the model files...')\n",
    "\n",
    "style_predict_path = tf.keras.utils.get_file('style_predict.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/predict/1?lite-format=tflite')\n",
    "style_transform_path = style_transform_path = tf.keras.utils.get_file('style_transform.tflite', 'https://tfhub.dev/sayakpaul/lite-model/arbitrary-image-stylization-inceptionv3/int8/transfer/1?lite-format=tflite')\n",
    "\n",
    "print('Model files downloaded...')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sBNjdwyXlx_B",
    "cellView": "form"
   },
   "source": [
    "#@title 6. Stylize image ðŸ¥\n",
    "\n",
    "content_blending_ratio = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "#@markdown You're encouraged to play with the different values of `content_blending_ratio`.\n",
    "\n",
    "def imshow(image, title=None):\n",
    "  if len(image.shape) > 3:\n",
    "    image = tf.squeeze(image, axis=0)\n",
    "\n",
    "  plt.imshow(image)\n",
    "  if title:\n",
    "    plt.title(title)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "imshow(preprocessed_content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "imshow(preprocessed_style_image, 'Style Image')\n",
    "\n",
    "# Function to run style prediction on preprocessed style image.\n",
    "def run_style_predict(preprocessed_style_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_predict_path)\n",
    "\n",
    "  # Set model input.\n",
    "  interpreter.allocate_tensors()\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.set_tensor(input_details[0][\"index\"], preprocessed_style_image)\n",
    "\n",
    "  # Calculate style bottleneck.\n",
    "  interpreter.invoke()\n",
    "  style_bottleneck = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return style_bottleneck\n",
    "\n",
    "# Calculate style bottleneck for the preprocessed style image.\n",
    "print('Calculating style bottleneck...')\n",
    "style_bottleneck = run_style_predict(preprocessed_style_image)\n",
    "print('Style Bottleneck Shape:', style_bottleneck.shape)\n",
    "print('Stylizing image. It should not take more than three minutes...')\n",
    "\n",
    "# Run style transform on preprocessed style image\n",
    "def run_style_transform(style_bottleneck, preprocessed_content_image):\n",
    "  # Load the model.\n",
    "  interpreter = tf.lite.Interpreter(model_path=style_transform_path)\n",
    "\n",
    "  # Set model input.\n",
    "  input_details = interpreter.get_input_details()\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  # Set model inputs.\n",
    "  for index in range(len(input_details)):\n",
    "    if input_details[index][\"name\"]=='Conv/BiasAdd':\n",
    "      interpreter.set_tensor(input_details[index][\"index\"], style_bottleneck)\n",
    "    elif input_details[index][\"name\"]=='content_image':\n",
    "      interpreter.set_tensor(input_details[index][\"index\"], preprocessed_content_image)\n",
    "  interpreter.invoke()\n",
    "\n",
    "  # Transform content image.\n",
    "  stylized_image = interpreter.tensor(\n",
    "      interpreter.get_output_details()[0][\"index\"]\n",
    "      )()\n",
    "\n",
    "  return stylized_image\n",
    "\n",
    "# Calculate style bottleneck of the content image.\n",
    "style_bottleneck_content = run_style_predict(\n",
    "    preprocess_image(content_image, 256)\n",
    ")\n",
    "\n",
    "# Blend the style bottleneck of style image and content image\n",
    "style_bottleneck_blended = content_blending_ratio * style_bottleneck_content \\\n",
    "                           + (1 - content_blending_ratio) * style_bottleneck\n",
    "\n",
    "# Stylize the content image using the style bottleneck.\n",
    "stylized_image = run_style_transform(style_bottleneck_blended, preprocessed_content_image)\n",
    "\n",
    "# Visualize the output.\n",
    "plt.subplot(1, 3, 3)\n",
    "imshow(stylized_image, 'Stylized Image')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X2T476eHzViZ",
    "cellView": "form"
   },
   "source": [
    "#@title 7. Save your results ðŸ’¥\n",
    "#@markdown If you experiment with the different `content_blending_ratio` values make sure you run this code block again in order to store your results online.\n",
    "\n",
    "style_image_resized = tf.image.resize(preprocessed_style_image, (preprocessed_content_image.shape[1], preprocessed_content_image.shape[2]))\n",
    "images = [preprocessed_content_image, style_image_resized, stylized_image]\n",
    "captions = [\"content_image\", \"style_image\", \"stylized_image\"]\n",
    "\n",
    "wandb.init(project='styletransfer', entity='wandb', anonymous='allow')\n",
    "wandb.log({\"Results\": [wandb.Image(tf.squeeze(image, 0), caption=caption)\n",
    "    for (image, caption) in zip(images, captions)]})\n",
    "display(wandb.jupyter.Run())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "efUfBlzN0o6y"
   },
   "source": [
    "Just click the link above that corresponds to `Run page` to see the results in a separate Browser tab. "
   ]
  }
 ]
}